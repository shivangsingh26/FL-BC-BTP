{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivangsingh26/FL-BC-BTP/blob/master/FedWPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEDWPR (5% WITHOUT HYPERPARAMETER TUNING)"
      ],
      "metadata": {
        "id": "n2yKIlVk7loB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y91GcZt3Jmww",
        "outputId": "73844d3f-fee4-493b-8613-2adfc4252b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting federated\n",
            "  Downloading federated-0.0.1-py3-none-any.whl (2.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: federated\n",
            "Successfully installed federated-0.0.1\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow federated\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bYVUKghKEeP",
        "outputId": "1b31e2b0-2c78-4c96-d650-baf1d0298d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Unzip the datasets\n",
        "zip_path = '/content/drive/My Drive/BTP.zip'\n",
        "unzip_path = '/content/datasets'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUrFEeapKcTJ",
        "outputId": "f21f858d-4ccd-421d-d9c1-89a2189c2af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19012 images belonging to 19 classes.\n",
            "Found 4748 images belonging to 19 classes.\n",
            "Found 19185 images belonging to 19 classes.\n",
            "Found 4791 images belonging to 19 classes.\n",
            "Found 19703 images belonging to 19 classes.\n",
            "Found 4921 images belonging to 19 classes.\n",
            "Found 19530 images belonging to 19 classes.\n",
            "Found 4878 images belonging to 19 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Function to load and preprocess data with augmentation\n",
        "def load_data(client_path):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.3,\n",
        "        height_shift_range=0.3,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.3,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        os.path.join(client_path, 'train'),\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        os.path.join(client_path, 'test'),\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "clients = [\"/content/datasets/Non-iid datasets/non_iid_subset_1\",\"/content/datasets/Non-iid datasets/non_iid_subset_2\",\"/content/datasets/Non-iid datasets/non_iid_subset_3\",\"/content/datasets/Non-iid datasets/non_iid_subset_4\",]\n",
        "data_paths = [os.path.join(unzip_path, client) for client in clients]\n",
        "\n",
        "train_generators = []\n",
        "test_generators = []\n",
        "for path in data_paths:\n",
        "    train_gen, test_gen = load_data(path)\n",
        "    train_generators.append(train_gen)\n",
        "    test_generators.append(test_gen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25fFqH7-K0_f"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes=19):\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                                   include_top=False,\n",
        "                                                   weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuj1zUnqK4nx",
        "outputId": "683473af-c047-446c-f268-9e6ef1bac94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RR = 0.3, epochs = 5\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 2s 0us/step\n",
            "Round 1/10\n",
            "Training client 1\n",
            "Epoch 1/5\n",
            "595/595 [==============================] - 322s 526ms/step - loss: 2.5215 - accuracy: 0.2138 - val_loss: 2.2394 - val_accuracy: 0.3317\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 266s 446ms/step - loss: 2.2101 - accuracy: 0.3029 - val_loss: 1.8453 - val_accuracy: 0.4164\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 263s 442ms/step - loss: 2.1064 - accuracy: 0.3348 - val_loss: 1.6908 - val_accuracy: 0.4558\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 262s 440ms/step - loss: 2.0058 - accuracy: 0.3617 - val_loss: 1.5987 - val_accuracy: 0.5065\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 265s 445ms/step - loss: 1.9645 - accuracy: 0.3811 - val_loss: 1.6173 - val_accuracy: 0.4941\n",
            "Training client 2\n",
            "Epoch 1/5\n",
            "600/600 [==============================] - 328s 539ms/step - loss: 2.3289 - accuracy: 0.2792 - val_loss: 1.6640 - val_accuracy: 0.4519\n",
            "Epoch 2/5\n",
            "600/600 [==============================] - 305s 509ms/step - loss: 1.9579 - accuracy: 0.3738 - val_loss: 1.3952 - val_accuracy: 0.5523\n",
            "Epoch 3/5\n",
            "600/600 [==============================] - 282s 470ms/step - loss: 1.8199 - accuracy: 0.4212 - val_loss: 1.3458 - val_accuracy: 0.5583\n",
            "Epoch 4/5\n",
            "600/600 [==============================] - 269s 448ms/step - loss: 1.7482 - accuracy: 0.4430 - val_loss: 1.1708 - val_accuracy: 0.6272\n",
            "Epoch 5/5\n",
            "600/600 [==============================] - 322s 537ms/step - loss: 1.6850 - accuracy: 0.4579 - val_loss: 1.1236 - val_accuracy: 0.6420\n",
            "Training client 3\n",
            "Epoch 1/5\n",
            "616/616 [==============================] - 369s 593ms/step - loss: 2.4799 - accuracy: 0.2336 - val_loss: 1.9068 - val_accuracy: 0.4152\n",
            "Epoch 2/5\n",
            "616/616 [==============================] - 373s 605ms/step - loss: 2.1480 - accuracy: 0.3202 - val_loss: 1.8261 - val_accuracy: 0.4215\n",
            "Epoch 3/5\n",
            "616/616 [==============================] - 348s 564ms/step - loss: 2.0310 - accuracy: 0.3517 - val_loss: 1.6683 - val_accuracy: 0.4597\n",
            "Epoch 4/5\n",
            "616/616 [==============================] - 332s 539ms/step - loss: 1.9649 - accuracy: 0.3755 - val_loss: 1.5965 - val_accuracy: 0.4765\n",
            "Epoch 5/5\n",
            "616/616 [==============================] - 288s 467ms/step - loss: 1.9042 - accuracy: 0.3969 - val_loss: 1.4709 - val_accuracy: 0.5216\n",
            "Training client 4\n",
            "Epoch 1/5\n",
            "611/611 [==============================] - 363s 588ms/step - loss: 2.4296 - accuracy: 0.2387 - val_loss: 2.0005 - val_accuracy: 0.3542\n",
            "Epoch 2/5\n",
            "611/611 [==============================] - 324s 530ms/step - loss: 2.1040 - accuracy: 0.3286 - val_loss: 1.8447 - val_accuracy: 0.4014\n",
            "Epoch 3/5\n",
            "611/611 [==============================] - 288s 472ms/step - loss: 1.9966 - accuracy: 0.3626 - val_loss: 1.9098 - val_accuracy: 0.3846\n",
            "Epoch 4/5\n",
            "611/611 [==============================] - 273s 447ms/step - loss: 1.9151 - accuracy: 0.3858 - val_loss: 1.5175 - val_accuracy: 0.4998\n",
            "Epoch 5/5\n",
            "611/611 [==============================] - 276s 451ms/step - loss: 1.8787 - accuracy: 0.4013 - val_loss: 1.4264 - val_accuracy: 0.5211\n",
            "Round 2/10\n",
            "Training client 1\n",
            "Epoch 1/5\n",
            "595/595 [==============================] - 333s 560ms/step - loss: 2.9453 - accuracy: 0.0536 - val_loss: 2.9376 - val_accuracy: 0.0545\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 307s 516ms/step - loss: 2.9372 - accuracy: 0.0485 - val_loss: 2.9363 - val_accuracy: 0.0545\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 275s 463ms/step - loss: 2.9365 - accuracy: 0.0517 - val_loss: 2.9360 - val_accuracy: 0.0545\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 267s 448ms/step - loss: 2.9364 - accuracy: 0.0514 - val_loss: 2.9359 - val_accuracy: 0.0545\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 268s 451ms/step - loss: 2.9363 - accuracy: 0.0522 - val_loss: 2.9359 - val_accuracy: 0.0545\n",
            "Training client 2\n",
            "Epoch 1/5\n",
            "600/600 [==============================] - 347s 578ms/step - loss: 2.9471 - accuracy: 0.0536 - val_loss: 2.9379 - val_accuracy: 0.0541\n",
            "Epoch 2/5\n",
            "600/600 [==============================] - 314s 524ms/step - loss: 2.9377 - accuracy: 0.0499 - val_loss: 2.9368 - val_accuracy: 0.0541\n",
            "Epoch 3/5\n",
            "600/600 [==============================] - 287s 478ms/step - loss: 2.9371 - accuracy: 0.0500 - val_loss: 2.9366 - val_accuracy: 0.0541\n",
            "Epoch 4/5\n",
            "600/600 [==============================] - 276s 460ms/step - loss: 2.9369 - accuracy: 0.0529 - val_loss: 2.9365 - val_accuracy: 0.0541\n",
            "Epoch 5/5\n",
            "600/600 [==============================] - 276s 460ms/step - loss: 2.9368 - accuracy: 0.0516 - val_loss: 2.9364 - val_accuracy: 0.0541\n",
            "Training client 3\n",
            "Epoch 1/5\n",
            "616/616 [==============================] - 350s 568ms/step - loss: 2.9564 - accuracy: 0.0524 - val_loss: 2.9448 - val_accuracy: 0.0526\n",
            "Epoch 2/5\n",
            "616/616 [==============================] - 327s 530ms/step - loss: 2.9449 - accuracy: 0.0509 - val_loss: 2.9445 - val_accuracy: 0.0526\n",
            "Epoch 3/5\n",
            "616/616 [==============================] - 295s 479ms/step - loss: 2.9448 - accuracy: 0.0508 - val_loss: 2.9445 - val_accuracy: 0.0526\n",
            "Epoch 4/5\n",
            "616/616 [==============================] - 281s 456ms/step - loss: 2.9448 - accuracy: 0.0498 - val_loss: 2.9445 - val_accuracy: 0.0526\n",
            "Epoch 5/5\n",
            "616/616 [==============================] - 289s 469ms/step - loss: 2.9448 - accuracy: 0.0496 - val_loss: 2.9444 - val_accuracy: 0.0526\n",
            "Training client 4\n",
            "Epoch 1/5\n",
            "611/611 [==============================] - 340s 556ms/step - loss: 2.9472 - accuracy: 0.0513 - val_loss: 2.9440 - val_accuracy: 0.0531\n",
            "Epoch 2/5\n",
            "611/611 [==============================] - 320s 524ms/step - loss: 2.9442 - accuracy: 0.0492 - val_loss: 2.9438 - val_accuracy: 0.0531\n",
            "Epoch 3/5\n",
            "611/611 [==============================] - 283s 463ms/step - loss: 2.9441 - accuracy: 0.0515 - val_loss: 2.9437 - val_accuracy: 0.0531\n",
            "Epoch 4/5\n",
            "611/611 [==============================] - 278s 455ms/step - loss: 2.9440 - accuracy: 0.0506 - val_loss: 2.9437 - val_accuracy: 0.0531\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define number of communication rounds and hyperparameters grid\n",
        "num_rounds = 5\n",
        "hyperparameters_grid = {\n",
        "    'RR': [0.3, 0.5, 0.7],\n",
        "    'epochs': [5, 10]\n",
        "}\n",
        "\n",
        "def fed_wpr(client_models, RR):\n",
        "    new_weights_list = []\n",
        "    client_weights = [model.get_weights() for model in client_models]\n",
        "\n",
        "    for client_id in range(len(client_models)):\n",
        "        new_weights = []\n",
        "        for layer_weights in zip(*client_weights):\n",
        "            weighted_sum = np.zeros_like(layer_weights[0])\n",
        "            for j in range(len(client_models)):\n",
        "                weighted_sum += RR * layer_weights[j]\n",
        "            personalized_weights = (1 - RR) * layer_weights[client_id] + weighted_sum\n",
        "            new_weights.append(personalized_weights)\n",
        "        new_weights_list.append(new_weights)\n",
        "\n",
        "    return new_weights_list\n",
        "\n",
        "def train_and_evaluate(client_models, train_generators, test_generators, RR, epochs, num_rounds=10):\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f'Round {round_num+1}/{num_rounds}')\n",
        "\n",
        "        for i in range(4):\n",
        "            print(f'Training client {i+1}')\n",
        "            client_models[i].fit(train_generators[i], epochs=epochs, validation_data=test_generators[i])\n",
        "\n",
        "        new_weights_list = fed_wpr(client_models, RR)\n",
        "\n",
        "        for i in range(4):\n",
        "            client_models[i].set_weights(new_weights_list[i])\n",
        "\n",
        "    avg_accuracy = 0\n",
        "    for i in range(4):\n",
        "        loss, accuracy = client_models[i].evaluate(test_generators[i])\n",
        "        avg_accuracy += accuracy\n",
        "        print(f'Client {i+1} - Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "    avg_accuracy /= 4\n",
        "    return avg_accuracy\n",
        "\n",
        "def grid_search(hyperparameters_grid):\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    for RR in hyperparameters_grid['RR']:\n",
        "        for epochs in hyperparameters_grid['epochs']:\n",
        "            print(f'Evaluating RR = {RR}, epochs = {epochs}')\n",
        "            client_models = [create_model(num_classes=19) for _ in range(4)]\n",
        "\n",
        "            avg_accuracy = train_and_evaluate(client_models, train_generators, test_generators, RR, epochs)\n",
        "\n",
        "            print(f'Average accuracy for RR = {RR}, epochs = {epochs}: {avg_accuracy}')\n",
        "\n",
        "            if avg_accuracy > best_accuracy:\n",
        "                best_accuracy = avg_accuracy\n",
        "                best_params = {'RR': RR, 'epochs': epochs}\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "best_params, best_accuracy = grid_search(hyperparameters_grid)\n",
        "print(f'Best parameters: {best_params} with average accuracy: {best_accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkpJ-RnaLWyL"
      },
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    loss, accuracy = client_models[i].evaluate(test_generators[i])\n",
        "    print(f'Client {i+1} - Loss: {loss}, Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEDWPR (5% WITH HYPERPARAMETER TUNING)"
      ],
      "metadata": {
        "id": "OjF_HoBU7uHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Unzip the datasets\n",
        "zip_path = '/content/drive/My Drive/BTP.zip'\n",
        "unzip_path = '/content/datasets'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM6Ox6kknsIt",
        "outputId": "ff354397-34a5-44f1-a9a3-4fbeee561075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMU2rdA0E4sY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bed518-556f-494f-8ee0-89b726452320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19012 images belonging to 19 classes.\n",
            "Found 4748 images belonging to 19 classes.\n",
            "Found 19185 images belonging to 19 classes.\n",
            "Found 4791 images belonging to 19 classes.\n",
            "Found 19703 images belonging to 19 classes.\n",
            "Found 4921 images belonging to 19 classes.\n",
            "Found 19530 images belonging to 19 classes.\n",
            "Found 4878 images belonging to 19 classes.\n",
            "Evaluating RR = 0.3, epochs = 10\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n",
            "Round 1/10\n",
            "Training client 1\n",
            "Epoch 1/10\n",
            "595/595 [==============================] - 347s 569ms/step - loss: 2.5186 - accuracy: 0.2171 - val_loss: 2.1193 - val_accuracy: 0.3370 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "595/595 [==============================] - 300s 504ms/step - loss: 2.2344 - accuracy: 0.2940 - val_loss: 1.8850 - val_accuracy: 0.4021 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "595/595 [==============================] - 274s 460ms/step - loss: 2.1054 - accuracy: 0.3365 - val_loss: 1.8559 - val_accuracy: 0.4185 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "595/595 [==============================] - 282s 473ms/step - loss: 2.0469 - accuracy: 0.3480 - val_loss: 1.7433 - val_accuracy: 0.4316 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "595/595 [==============================] - 271s 454ms/step - loss: 1.9915 - accuracy: 0.3692 - val_loss: 1.5714 - val_accuracy: 0.4987 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "595/595 [==============================] - 269s 453ms/step - loss: 1.9290 - accuracy: 0.3882 - val_loss: 1.4175 - val_accuracy: 0.5497 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "595/595 [==============================] - 271s 456ms/step - loss: 1.8877 - accuracy: 0.4015 - val_loss: 1.4377 - val_accuracy: 0.5453 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.8582 - accuracy: 0.4081\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "595/595 [==============================] - 270s 453ms/step - loss: 1.8582 - accuracy: 0.4081 - val_loss: 1.4386 - val_accuracy: 0.5581 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "595/595 [==============================] - 270s 454ms/step - loss: 1.7517 - accuracy: 0.4420 - val_loss: 1.4007 - val_accuracy: 0.5640 - lr: 5.0000e-04\n",
            "Epoch 10/10\n",
            "595/595 [==============================] - 271s 456ms/step - loss: 1.7261 - accuracy: 0.4514 - val_loss: 1.3701 - val_accuracy: 0.5758 - lr: 5.0000e-04\n",
            "Training client 2\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 342s 565ms/step - loss: 2.3349 - accuracy: 0.2686 - val_loss: 1.6200 - val_accuracy: 0.4943 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 321s 535ms/step - loss: 1.9554 - accuracy: 0.3758 - val_loss: 1.4782 - val_accuracy: 0.5197 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 292s 488ms/step - loss: 1.8288 - accuracy: 0.4129 - val_loss: 1.4808 - val_accuracy: 0.5110 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 270s 451ms/step - loss: 1.7617 - accuracy: 0.4391 - val_loss: 1.2903 - val_accuracy: 0.5640 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 272s 454ms/step - loss: 1.6947 - accuracy: 0.4541 - val_loss: 1.3509 - val_accuracy: 0.5652 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 272s 453ms/step - loss: 1.6457 - accuracy: 0.4715 - val_loss: 1.1935 - val_accuracy: 0.6091 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 275s 459ms/step - loss: 1.6109 - accuracy: 0.4826 - val_loss: 1.0550 - val_accuracy: 0.6625 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 272s 454ms/step - loss: 1.5577 - accuracy: 0.5015 - val_loss: 0.9747 - val_accuracy: 0.6932 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 275s 459ms/step - loss: 1.5495 - accuracy: 0.5024 - val_loss: 1.0012 - val_accuracy: 0.6673 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - ETA: 0s - loss: 1.5189 - accuracy: 0.5092\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "600/600 [==============================] - 270s 451ms/step - loss: 1.5189 - accuracy: 0.5092 - val_loss: 0.9926 - val_accuracy: 0.6794 - lr: 0.0010\n",
            "Training client 3\n",
            "Epoch 1/10\n",
            "616/616 [==============================] - 374s 603ms/step - loss: 2.4774 - accuracy: 0.2333 - val_loss: 1.9446 - val_accuracy: 0.4280 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "616/616 [==============================] - 343s 557ms/step - loss: 2.1514 - accuracy: 0.3215 - val_loss: 1.6851 - val_accuracy: 0.4503 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "616/616 [==============================] - 331s 537ms/step - loss: 2.0395 - accuracy: 0.3508 - val_loss: 1.5742 - val_accuracy: 0.4682 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "616/616 [==============================] - 301s 488ms/step - loss: 1.9682 - accuracy: 0.3703 - val_loss: 1.6092 - val_accuracy: 0.4603 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "616/616 [==============================] - 288s 468ms/step - loss: 1.9113 - accuracy: 0.3956 - val_loss: 1.4124 - val_accuracy: 0.5312 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "616/616 [==============================] - 281s 457ms/step - loss: 1.8568 - accuracy: 0.4078 - val_loss: 1.3681 - val_accuracy: 0.5525 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "616/616 [==============================] - 291s 472ms/step - loss: 1.8253 - accuracy: 0.4176 - val_loss: 1.3785 - val_accuracy: 0.5367 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "146/616 [======>.......................] - ETA: 3:23 - loss: 1.8332 - accuracy: 0.4110"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Function to load and preprocess data with augmentation\n",
        "def load_data(client_path):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.3,\n",
        "        height_shift_range=0.3,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.3,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        os.path.join(client_path, 'train'),\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        os.path.join(client_path, 'test'),\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "clients = [\"/content/datasets/Non-iid datasets/non_iid_subset_1\",\"/content/datasets/Non-iid datasets/non_iid_subset_2\",\"/content/datasets/Non-iid datasets/non_iid_subset_3\",\"/content/datasets/Non-iid datasets/non_iid_subset_4\",]\n",
        "data_paths = [os.path.join(unzip_path, client) for client in clients]\n",
        "\n",
        "train_generators = []\n",
        "test_generators = []\n",
        "for path in data_paths:\n",
        "    train_gen, test_gen = load_data(path)\n",
        "    train_generators.append(train_gen)\n",
        "    test_generators.append(test_gen)\n",
        "\n",
        "def create_model(num_classes=19):\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                                   include_top=False,\n",
        "                                                   weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the FedWPR aggregation function\n",
        "def fed_wpr(client_models, RR):\n",
        "    new_weights_list = []\n",
        "    client_weights = [model.get_weights() for model in client_models]\n",
        "\n",
        "    for client_id in range(len(client_models)):\n",
        "        new_weights = []\n",
        "        for layer_weights in zip(*client_weights):\n",
        "            weighted_sum = np.sum([w * RR for w in layer_weights], axis=0)\n",
        "            personalized_weights = (1 - RR) * layer_weights[client_id] + weighted_sum\n",
        "            new_weights.append(personalized_weights)\n",
        "        new_weights_list.append(new_weights)\n",
        "\n",
        "    return new_weights_list\n",
        "\n",
        "# Train and evaluate function with learning rate scheduling and early stopping\n",
        "def train_and_evaluate(client_models, train_generators, test_generators, RR, epochs, num_rounds=10):\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f'Round {round_num+1}/{num_rounds}')\n",
        "\n",
        "        for i in range(4):\n",
        "            print(f'Training client {i+1}')\n",
        "            client_models[i].fit(train_generators[i], epochs=epochs, validation_data=test_generators[i],\n",
        "                                 callbacks=[callback, lr_scheduler])\n",
        "\n",
        "        new_weights_list = fed_wpr(client_models, RR)\n",
        "\n",
        "        for i in range(4):\n",
        "            client_models[i].set_weights(new_weights_list[i])\n",
        "\n",
        "    avg_accuracy = 0\n",
        "    for i in range(4):\n",
        "        loss, accuracy = client_models[i].evaluate(test_generators[i])\n",
        "        avg_accuracy += accuracy\n",
        "        print(f'Client {i+1} - Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "    avg_accuracy /= 4\n",
        "    return avg_accuracy\n",
        "\n",
        "# Custom grid search function with detailed monitoring\n",
        "def grid_search(hyperparameters_grid):\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    param_grid = list(ParameterGrid(hyperparameters_grid))\n",
        "\n",
        "    for params in param_grid:\n",
        "        RR = params['RR']\n",
        "        epochs = params['epochs']\n",
        "\n",
        "        print(f'Evaluating RR = {RR}, epochs = {epochs}')\n",
        "        client_models = [create_model(num_classes=19) for _ in range(4)]\n",
        "\n",
        "        avg_accuracy = train_and_evaluate(client_models, train_generators, test_generators, RR, epochs)\n",
        "\n",
        "        print(f'Average accuracy for RR = {RR}, epochs = {epochs}: {avg_accuracy}')\n",
        "\n",
        "        if avg_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_accuracy\n",
        "            best_params = {'RR': RR, 'epochs': epochs}\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "# Define hyperparameters grid\n",
        "hyperparameters_grid = {\n",
        "    'RR': [0.3, 0.5, 0.7],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "best_params, best_accuracy = grid_search(hyperparameters_grid)\n",
        "print(f'Best parameters: {best_params} with average accuracy: {best_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEDWPR(NEW)"
      ],
      "metadata": {
        "id": "07H_Htky75FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "# !pip install tensorflow\n",
        "!pip install tensorflow_federated\n"
      ],
      "metadata": {
        "id": "ihuIxiGQnt0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d16961bb-c5a2-438e-ccd2-63dd1b645fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting tensorflow_federated\n",
            "  Downloading tensorflow_federated-0.82.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.4.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (23.2.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (5.3.3)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow_federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-vizier==0.1.11 (from tensorflow_federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.64.1)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow_federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax==0.4.14 (from tensorflow_federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.25.2)\n",
            "Collecting portpicker~=1.6 (from tensorflow_federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow_federated)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow_federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-privacy==0.9.0 (from tensorflow_federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.14.*,>=2.14.0 (from tensorflow_federated)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (4.66.4)\n",
            "Collecting typing-extensions==4.5.*,>=4.5.0 (from tensorflow_federated)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow_federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow_federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow_federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow_federated) (3.20.3)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow_federated) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.37.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow_federated) (1.2.2)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow_federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow_federated) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.43.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf>=3.6 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow_federated) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (2.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.2.2)\n",
            "Building wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535361 sha256=a2ed47a47902ec0a2b75f49a4983c1090dd89ab2e58702c447306e118bf0e416\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529855 sha256=873de06dd667661ce2b9fa5db584b300cc71ed4e8a0d21526ca4e1a773ffba17\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: typing-extensions, tensorflow-probability, tensorflow-model-optimization, tensorflow-estimator, sqlalchemy, scipy, protobuf, portpicker, packaging, keras, attrs, jaxlib, jax, grpcio-tools, googleapis-common-protos, dp-accounting, google-vizier, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-privacy, tensorflow_federated\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.23.0\n",
            "    Uninstalling tensorflow-probability-0.23.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.23.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.31\n",
            "    Uninstalling SQLAlchemy-2.0.31:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.31\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.63.2\n",
            "    Uninstalling googleapis-common-protos-1.63.2:\n",
            "      Successfully uninstalled googleapis-common-protos-1.63.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.0+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "chex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\n",
            "flax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "pydantic 2.8.0 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.20.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.3.0+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.2 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 packaging-22.0 portpicker-1.6.0 protobuf-4.25.3 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 tensorflow_federated-0.82.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "portpicker"
                ]
              },
              "id": "c8981430cf484ecb8b829f68c105baf7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Extract the dataset\n",
        "zip_path = '/content/drive/MyDrive/BTP.zip'\n",
        "extract_path = '/content/finger_vein_dataset'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "c-dXBaOO8ZBo",
        "outputId": "5e040688-b66d-499b-fc96-3e6faebdb367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ecc96be6461d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create data loaders for each client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m   1005\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for each client\n",
        "data_dir = extract_path\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "def create_data_loader(data_path, img_height, img_width, batch_size):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    data = datagen.flow_from_directory(\n",
        "        data_path,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "    return data\n",
        "\n",
        "clients_train_data = []\n",
        "clients_test_data = []\n",
        "for i in range(1, 5):\n",
        "    client_train_path = os.path.join(data_dir, f'/content/finger_vein_dataset/Non-iid datasets/non_iid_subset_{i}', 'train')\n",
        "    client_test_path = os.path.join(data_dir, f'/content/finger_vein_dataset/Non-iid datasets/non_iid_subset_{i}', 'test')\n",
        "    train_data = create_data_loader(client_train_path, img_height, img_width, batch_size)\n",
        "    test_data = create_data_loader(client_test_path, img_height, img_width, batch_size)\n",
        "    clients_train_data.append(train_data)\n",
        "    clients_test_data.append(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiQStgYl_-j8",
        "outputId": "8b64418b-ab45-47e7-b2e0-a10b50e7c957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19012 images belonging to 19 classes.\n",
            "Found 4748 images belonging to 19 classes.\n",
            "Found 19185 images belonging to 19 classes.\n",
            "Found 4791 images belonging to 19 classes.\n",
            "Found 19703 images belonging to 19 classes.\n",
            "Found 4921 images belonging to 19 classes.\n",
            "Found 19530 images belonging to 19 classes.\n",
            "Found 4878 images belonging to 19 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "fdJwpoIT-U2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Client training function\n",
        "def client_train(model, train_data, epochs=1):\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_data, epochs=epochs, verbose=1)\n",
        "    return model.get_weights()\n",
        "\n",
        "# FedWPR aggregation function\n",
        "def fedwpr_aggregation(local_weights, RR):\n",
        "    N = len(local_weights)\n",
        "    new_weights = []\n",
        "\n",
        "    for layer_weights in zip(*local_weights):\n",
        "        new_layer_weights = np.zeros_like(layer_weights[0])\n",
        "        for i in range(N):\n",
        "            new_layer_weights += RR * np.array(layer_weights[i]) / N\n",
        "        for i in range(N):\n",
        "            new_layer_weights += (1 - RR) * np.array(layer_weights[i]) / N\n",
        "        new_weights.append(new_layer_weights)\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "def fedwpr_aggregation(local_weights, RR):\n",
        "    N = len(local_weights)\n",
        "    aggregated_weights = []\n",
        "\n",
        "    for layer_weights in zip(*local_weights):\n",
        "        rr_component = np.zeros_like(layer_weights[0])\n",
        "        one_minus_rr_component = np.zeros_like(layer_weights[0])\n",
        "\n",
        "        # Calculate RR component\n",
        "        for weights in layer_weights:\n",
        "            rr_component += RR * np.array(weights) / N\n",
        "\n",
        "        # Calculate (1 - RR) component\n",
        "        for weights in layer_weights:\n",
        "            one_minus_rr_component += (1 - RR) * np.array(weights) / N\n",
        "\n",
        "        # Combine both components\n",
        "        new_layer_weights = rr_component + one_minus_rr_component\n",
        "        aggregated_weights.append(new_layer_weights)\n",
        "\n",
        "    return aggregated_weights\n",
        "\n",
        "# Simulate federated learning with FedFV and FedWPR\n",
        "global_model = create_model(num_classes=19)  # Assuming 20 classes for the global model\n",
        "num_rounds = 10\n",
        "RR = 0.5  # Set the RR value as needed\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    local_weights = []\n",
        "    for client_num, train_data in enumerate(clients_train_data):\n",
        "        client_model = create_model(num_classes=train_data.num_classes)\n",
        "        client_model.set_weights(global_model.get_weights())\n",
        "        client_weights = client_train(client_model, train_data, epochs=1)\n",
        "        local_weights.append(client_weights)\n",
        "\n",
        "    # Apply FedWPR aggregation\n",
        "    new_global_weights = fedwpr_aggregation(local_weights, RR)\n",
        "    global_model.set_weights(new_global_weights)\n",
        "\n",
        "    # Distribute the aggregated model back to clients\n",
        "    for client_num, train_data in enumerate(clients_train_data):\n",
        "        client_model.set_weights(new_global_weights)\n",
        "\n",
        "    print(f'Round {round_num + 1} completed')\n",
        "\n",
        "# Save the global model\n",
        "global_model.save('/content/drive/MyDrive/federated_global_model_fedwpr.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8NyKi10-1Fb",
        "outputId": "855c8b21-7e90-42b5-9017-df058f5346a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "595/595 [==============================] - 619s 1s/step - loss: 0.7979 - accuracy: 0.7477\n",
            "600/600 [==============================] - 619s 1s/step - loss: 0.5765 - accuracy: 0.8236\n",
            "616/616 [==============================] - 652s 1s/step - loss: 0.7054 - accuracy: 0.7814\n",
            "533/611 [=========================>....] - ETA: 1:21 - loss: 0.7660 - accuracy: 0.7630"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data):\n",
        "    loss, accuracy = model.evaluate(test_data, verbose=0)\n",
        "    return loss, accuracy\n",
        "\n",
        "# Evaluate the global model on each client's test data\n",
        "for client_num, test_data in enumerate(clients_test_data):\n",
        "    loss, accuracy = evaluate_model(global_model, test_data)\n",
        "    print(f'Client {client_num + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "v1lzttEe-2Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"OpenCV version:\", cv2.__version__)\n",
        "\n",
        "# Set the paths\n",
        "data_dir = 'Non-iid datasets'\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "num_classes = 19  # Number of classes\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    img = cv2.imread(path.decode('utf-8'))  # path needs to be decoded from bytes\n",
        "    img = cv2.resize(img, (img_width, img_height))\n",
        "    img = img / 255.0  # Normalize to [0, 1]\n",
        "    label = tf.one_hot(label, num_classes)  # One-hot encode the label\n",
        "    return img.astype(np.float32), label.numpy().astype(np.float32)\n",
        "\n",
        "def create_dataset(data_dir, batch_size):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(data_dir)\n",
        "\n",
        "    for label, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            image_paths.append(img_path)\n",
        "            labels.append(label)\n",
        "\n",
        "    image_paths = np.array(image_paths)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    dataset = Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: tf.numpy_function(load_and_preprocess_image, [x, y], [tf.float32, tf.float32]))\n",
        "\n",
        "    # Explicitly set the shapes\n",
        "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x, [img_height, img_width, 3]), tf.ensure_shape(y, [num_classes])))\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset, len(class_names)\n",
        "\n",
        "clients_train_data = []\n",
        "clients_test_data = []\n",
        "\n",
        "for i in range(1, 5):  # Adjusted to 4 clients\n",
        "    client_train_path = os.path.join(data_dir, f'client_{i}', 'train')\n",
        "    client_test_path = os.path.join(data_dir, f'client_{i}', 'test')\n",
        "    train_data, _ = create_dataset(client_train_path, batch_size)\n",
        "    test_data, _ = create_dataset(client_test_path, batch_size)\n",
        "    clients_train_data.append(train_data)\n",
        "    clients_test_data.append(test_data)\n",
        "\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    return model\n",
        "\n",
        "def client_train(model, train_data, client_num, epochs=1):\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(f\"Compiling model for client {client_num + 1}\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Client {client_num + 1}, Epoch {epoch + 1}/{epochs}\")\n",
        "        model.fit(train_data, epochs=1, verbose=1)\n",
        "    return model.get_weights()\n",
        "\n",
        "def fedwpr_aggregation(local_weights, RR):\n",
        "    N = len(local_weights)\n",
        "    aggregated_weights = []\n",
        "\n",
        "    for layer_weights in zip(*local_weights):\n",
        "        rr_component = np.zeros_like(layer_weights[0])\n",
        "        one_minus_rr_component = np.zeros_like(layer_weights[0])\n",
        "\n",
        "        # Calculate RR component\n",
        "        for weights in layer_weights:\n",
        "            rr_component += RR * np.array(weights) / N\n",
        "\n",
        "        # Calculate (1 - RR) component\n",
        "        for weights in layer_weights:\n",
        "            one_minus_rr_component += (1 - RR) * np.array(weights) / N\n",
        "\n",
        "        # Combine both components\n",
        "        new_layer_weights = rr_component + one_minus_rr_component\n",
        "        aggregated_weights.append(new_layer_weights)\n",
        "\n",
        "    return aggregated_weights\n",
        "\n",
        "# Simulate federated learning with FedFV and FedWPR\n",
        "global_model = create_model(num_classes=num_classes)  # Each client has 19 classes\n",
        "num_rounds = 1\n",
        "num_epochs = 1  # Set the number of epochs for each client per round\n",
        "RR = 0.5  # Set the RR value as needed\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    print(f\"Federated Round {round_num + 1}/{num_rounds}\")\n",
        "    local_weights = []\n",
        "    for client_num, train_data in enumerate(clients_train_data):\n",
        "        print(f\"Training Client {client_num + 1}\")\n",
        "        client_model = create_model(num_classes=num_classes)\n",
        "        client_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])  # Ensure the model is compiled before setting weights\n",
        "        client_model.set_weights(global_model.get_weights())\n",
        "        client_weights = client_train(client_model, train_data, client_num, epochs=num_epochs)\n",
        "        local_weights.append(client_weights)\n",
        "\n",
        "    # Apply FedWPR aggregation\n",
        "    print(\"Aggregating local models with FedWPR\")\n",
        "    new_global_weights = fedwpr_aggregation(local_weights, RR)\n",
        "    global_model.set_weights(new_global_weights)\n",
        "\n",
        "    # Distribute the aggregated model back to clients\n",
        "    for client_num, train_data in enumerate(clients_train_data):\n",
        "        client_model.set_weights(new_global_weights)\n",
        "\n",
        "    print(f\"Round {round_num + 1} completed\")\n",
        "\n",
        "# Save the global model\n",
        "global_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])  # Ensure the global model is compiled\n",
        "global_model.save('federated_global_model_fedwpr.h5')\n",
        "\n",
        "def evaluate_model(model, test_data):\n",
        "    loss, accuracy = model.evaluate(test_data, verbose=0)\n",
        "    return loss, accuracy\n",
        "\n",
        "# Evaluate the global model on each client's test data\n",
        "for client_num, test_data in enumerate(clients_test_data):\n",
        "    loss, accuracy = evaluate_model(global_model, test_data)\n",
        "    print(f'Client {client_num + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "gCB-QlQhVZY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeiusqstY9Jvxevj91KHjR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}